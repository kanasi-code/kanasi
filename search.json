[{"title":"拆解http keep-alive","url":"/2019/07/23/拆解http-keep-alive/","content":"# 拆解Http keep-alive\n\n最近正好在研究okhttp 的实现，我们知道okhttp针对http 1.1 及以上版本，是默认开启keepalive的，当开启keepalive之后，新的http请求会复用之前的连接用于发送数据接收响应，那么，这个所谓的复用连接在服务端到底是怎么实现的呢？我们来分析一下\n\n\n\n### 服务端如何实现keepalive？（以spring boot 内嵌的tomcat为例）\n\n\n\n#### 先来看看服务端如何处理一个新的http请求的：\n\n```\nNioEndpoint$Acceptor.run  //轮询处理socket连接，交给processor线程去处理\n  - NioEndpoint$Socketprocessor.doRun  //处理socket read事件，并基于处理完成返回的state处理socket\n    - AbstractProtocol$ConnectionHandler.process //根据协议选择对应的processor，并基于返回的state状态处理processor\n      - AbstractProcessorLight.process\n        - Http11Processor.service //实际的协议处理类，会调用CoyoteAdapter.service拿到reponse，并基于返回值设置response code \n          - CoyoteAdapter.service\n          .\n          .\n          .\n```\n\n\n\n#### 哪里处理socket状态呢？\n\n从上面的流程中，很容易看出，针对socket的处理实际上是在`NioEndpoint$Socketprocessor.doRun` 中做的，看看具体实现：\n\n```\n                if (handshake == 0) {\n                    SocketState state = SocketState.OPEN;\n                    // Process the request from this socket\n                    if (event == null) {\n                        state = getHandler().process(socketWrapper, SocketEvent.OPEN_READ);\n                    } else {\n                        state = getHandler().process(socketWrapper, event);\n                    }\n                    if (state == SocketState.CLOSED) {\n                        close(socket, key);\n                    }\n                } else if (handshake == -1 ) {\n                    close(socket, key);\n                } else if (handshake == SelectionKey.OP_READ){\n                    socketWrapper.registerReadInterest();\n                } else if (handshake == SelectionKey.OP_WRITE){\n                    socketWrapper.registerWriteInterest();\n                }\n```\n\n从以上代码可以看出，当state 为`SocketState.CLOSED` 时，服务端会主动close Socket，那么close(socket, key)又做了什么事呢？\n\n```\nprivate void close(NioChannel socket, SelectionKey key) {\n        try {\n            if (socket.getPoller().cancelledKey(key) != null) {\n                // SocketWrapper (attachment) was removed from the\n                // key - recycle the key. This can only happen once\n                // per attempted closure so it is used to determine\n                // whether or not to return the key to the cache.\n                // We do NOT want to do this more than once - see BZ\n                // 57340 / 57943.\n                if (running && !paused) {\n                    if (!nioChannels.push(socket)) {\n                        socket.free();\n                    }\n                }\n            }\n        } catch (Exception x) {\n            log.error(\"\",x);\n        }\n    }\n```\n\n其实主要是调用了cancelledKey(key)方法，这个方法主要的作用就是调用channel 的close方法，关闭channel。\n\n\n\n#### 什么情况下socket 会返回SocketState.CLOSED？\n\n从上面的分析，我们知道，当处理线程返回SocketState.CLOSED的时候，客户端与服务端之间建立的channel就会被关闭。那么， 什么情况下，处理线程会反馈SocketState.CLOSED呢？来看看Http11Processor.service方法：\n\n```\n  @Override\n    public SocketState service(SocketWrapperBase<?> socketWrapper)\n        throws IOException {\n        .\n        .\n        .\n            // Process the request in the adapter\n            if (!getErrorState().isError()) {\n                try {\n                    rp.setStage(org.apache.coyote.Constants.STAGE_SERVICE);\n                    getAdapter().service(request, response);\n                    // Handle when the response was committed before a serious\n                    // error occurred.  Throwing a ServletException should both\n                    // set the status to 500 and set the errorException.\n                    // If we fail here, then the response is likely already\n                    // committed, so we can't try and set headers.\n                    if(keepAlive && !getErrorState().isError() && !isAsync() &&\n                            statusDropsConnection(response.getStatus())) {\n                        setErrorState(ErrorState.CLOSE_CLEAN, null);\n                    }\n                } catch (InterruptedIOException e) {\n                    setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e);\n                } catch (HeadersTooLargeException e) {\n                    log.error(sm.getString(\"http11processor.request.process\"), e);\n                    // The response should not have been committed but check it\n                    // anyway to be safe\n                    if (response.isCommitted()) {\n                        setErrorState(ErrorState.CLOSE_NOW, e);\n                    } else {\n                        response.reset();\n                        response.setStatus(500);\n                        setErrorState(ErrorState.CLOSE_CLEAN, e);\n                        response.setHeader(\"Connection\", \"close\"); // TODO: Remove\n                    }\n                } catch (Throwable t) {\n                    ExceptionUtils.handleThrowable(t);\n                    log.error(sm.getString(\"http11processor.request.process\"), t);\n                    // 500 - Internal Server Error\n                    response.setStatus(500);\n                    setErrorState(ErrorState.CLOSE_CLEAN, t);\n                    getAdapter().log(request, response, 0);\n                }\n            }\n\t\t\t\t\t\t.\n\t\t\t\t\t\t.\n\t\t\t\t\t\t.\n        if (getErrorState().isError() || endpoint.isPaused()) {\n            return SocketState.CLOSED;\n        } else if (isAsync()) {\n            return SocketState.LONG;\n        } else if (isUpgrade()) {\n            return SocketState.UPGRADING;\n        } else {\n            if (sendfileState == SendfileState.PENDING) {\n                return SocketState.SENDFILE;\n            } else {\n                if (openSocket) {\n                    if (readComplete) {\n                        return SocketState.OPEN;\n                    } else {\n                        return SocketState.LONG;\n                    }\n                } else {\n                    return SocketState.CLOSED;\n                }\n            }\n        }\n    }\n\n```\n\n在服务端处理没有error的情况下，响应会走到最下面的else里面，可以看到，主要是基于\n\nopenSocket来判断返回的socketstate，而openSocket的状态实际上是由keepAlive的值决定的；SO，实际上，服务端就是基于keepalive的值来判断是否在当次请求结束后关闭连接管道。\n\n\n\n#### 如果不关闭，服务端会做什么呢？\n\n从上面的代码可以看到，会有两种情况：当次请求数据已读取完成、当次请求数据读取未完成\n\n- 当次请求数据已经读取完成\n\n  >  返回SocketState.OPEN， 从connections中移除当前socket对应的Processor，并重新给当前socket注册读事件监听\n\n- 当次请求数据读取未完成\n\n  > 返回SocketState.LONG，保留socket对应的Processor，并继续为该socket注册读事件监听\n\n当服务端保留了socket，在未超时的情况，下一次同一个客户端请求过来，就不需要再进行三次握手了。\n\n\n\n### 客户端又是如何处理keep-alive呢？\n\n> 我们以okhttp client 为例，来看看客户端是怎么处理的\n\n如果你了解过okhttp的实现，应该知道okhttp client 的设计是责任链模式。其与服务端的连接建立实际是由ConnectInterceptor 处理的，它会返回一个RealConnection 交给接下来的interceptor去处理。核心处理逻辑是在ExchangeFinder中：\n\n```\n  @Throws(IOException::class)\n  private fun findConnection(\n    connectTimeout: Int,\n    readTimeout: Int,\n    writeTimeout: Int,\n    pingIntervalMillis: Int,\n    connectionRetryEnabled: Boolean\n  ): RealConnection {\n   \t\n    ...\n\n      if (result == null) {\n        // Attempt to get a connection from the pool.\n        if (connectionPool.transmitterAcquirePooledConnection(address, transmitter, null, false)) {\n          foundPooledConnection = true\n          result = transmitter.connection\n        } else if (nextRouteToTry != null) {\n          selectedRoute = nextRouteToTry\n          nextRouteToTry = null\n        } else if (retryCurrentRoute()) {\n          selectedRoute = transmitter.connection!!.route()\n        }\n      }\n    }\n    toClose?.closeQuietly()\n\n    if (releasedConnection != null) {\n      eventListener.connectionReleased(call, releasedConnection!!)\n    }\n    if (foundPooledConnection) {\n      eventListener.connectionAcquired(call, result!!)\n    }\n    if (result != null) {\n      // If we found an already-allocated or pooled connection, we're done.\n      return result!!\n    }\n\n\t\t...\n\n\n    // Do TCP + TLS handshakes. This is a blocking operation.\n    result!!.connect(\n        connectTimeout,\n        readTimeout,\n        writeTimeout,\n        pingIntervalMillis,\n        connectionRetryEnabled,\n        call,\n        eventListener\n    )\n\t\t\n\t\t...\n    \n    return result!!\n  }\n```\n\n切入到transmitterAcquirePooledConnection方法可以看到，如果请求的Address 一样，且有 有效connection 的情况下，默认会返回上一次的连接， 这样就打到了连接复用的目的，也就不需要再进行三次握手。\n\n那么， connection什么时候会被删除呢？\n\n这个就涉及到okhttp的连接管理机制，默认有几种情况：\n\n1、如果连接是非keep-alive模式的，则服务端会在返回头中带上Connection:close（默认500等场景下也会返回close），此时会直接关闭连接，即从connectionPool中删除connection；\n\n2、在keep-alive超时的情况下，也会从connectionPool中删除connection；\n\n3、超过最大空闲连接数量时，从connectionPool中删除connection；\n\n#### by the way\n\n有人可能会担心说，如果默认开启keep-alive连接复用，会不会导致请求会串行发出去呢，其实不会，在okhttp client中，可以设置默认直接支持的连接数，然后他会判断放点已经建立的连接是否是可用状态，比如未读取完成，未传输完成等，都是不能给相同地址的请求复用的，这个时候就会默认新建一个连接。\n\n"},{"title":"kubernetes 学习笔记","url":"/2019/07/16/kubernetes-学习笔记/","content":"# 大纲\n\n## kubernetes 介绍\n\n#### kubernetes 集群架构\n\n- master 节点\n\n  > 控制和管理整个集群系统的控制面板\n\n  - Kubernetes API 服务器\n    - Scheduler  调度器，用于调度你的应用\n  - Controller Manager 执行集群级别的功能，如复制组建、持续跟踪工作节点、处理失败节点\n  - etcd 可靠的分布式存储，用于持久化集群配置\n\n- worker 节点\n\n  > 运行容器化应用的机器 \n\n  - kubelet，与API服务器通信，并管理它所在节点的容器\n  - Docker/rtk 容器应用\n  - kube-proxy 负责组件之间的负载均衡网络流量\n\n#### kubernetes 的好处\n\n- 简化应用程序部署\n- \b更好的利用硬件\n- 健康检查和自修复\n- 自动扩容\n\n## 核心组件\n\n### Service\n\n- ClusterIP\n\n  > Kubernetes 的默认服务，定义一个集群内的服务，集群内的其他应用都可以访问该服务。集群外部无法访问它\n\n- LoadBalancer\n\n  > LoadBalancer 服务是暴露服务到internet的标准方式。通过在集群中创建一个负载均衡器来暴露对应服务，它将给你一个单独IP，转发所有流量到你的服务。\n\n- NodePort\n\n  > 将外部流量引导到内部服务的最原始方式，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都会被转发到对应服务\n\n##### 列出所有service\n\n> k get services\n\n\n\n### pod\n\n> 一个pod是一组紧密相关的容器，它们总是一起运行在同一个工作节点上，以及同一个Linux命名空间中，每个pod就像一个独立的逻辑机器，拥有自己的IP、主机名、进程等，运行一个独立的应用程序。\n\n##### 相关命令\n\n> k get pods  //获取pods\n>\n> k logs podname  // 获取指定pod的日志\n\n##### 何时在pod中使用多个容器？\n\n- 他们必须要一起运行\n- 他们代表着一个整体的组件\n- 他们必须一起进行扩容缩容\n\n##### yaml\n\n```yaml\napiVersion: V1\nkind: Pod\nmetadata: \n  name: kubia-manual\nspac:\n  containers:\n  - images: luksa/kubia\n    name: kubia\n    ports:\n    - containerPort: 8080\n    protocol: TCP\n```\n\n##### 标签\n\n> 标签是可以附加到资源的任意键值对，用以选择具有该确切标签的资源，用于快读区分出一组有共同特征的资源\n\n- 命令\n\n  > k get pods  - -show-labels //显示pod 对应的标签\n  >\n  > k get pods -l app=pbkd-user-material-task-pbkd //基于特定标签筛选pod\n\n\n\n##### 命令空间\n\n- 命令\n\n  > k  get ns // 列出所有命名空间\n  >\n  > k get pods -n pbkd // 基于命名空间查找pod\n\n- yaml\n\n  ```\n  apiVersion: V1\n  kind: Namespace\n  metadata: \n    name: kubia-manual\n  ```\n\n\n\n##### 停止和移除pod\n\n- 命令\n\n  > k delete pod kubia  //删除单个pod\n  >\n  > k delete pod - - all  //删除命名空间的所有pod\n  >\n  > k delete add - - all //删除所有资源\n\n##### 存活探针（liveness probe）\n\n> kubernetes 通过存活探针检查容器是否还在运行，可以为pod中的每个容器单独指定存活探针，如果检测失败，kubernetes将定期执行探针并重新启动容器\n\n- 三种探测容器的机制\n  - HTTP GET探针，对容器的IP地址执行HTTP GET请求，如果探测器收到响应，并且响应状态码<400 则代表成功，返回状态码错误或没有返回，则代表失败，重新启动容器\n  - TCP套接字探针尝试与容器指定端口建立TCP连接，如果连接成功建立则探测成功。否则容器重新启动\n  - Exec 探针在容器内执行任意命令，并检查命令的退出码。如果状态码是0 则探测成功。\n\n##### ReplicaSet\n\n> 是一种kubernetes 资源，用户确保pod始终保持运行状态，如果他维护的pod因任何原因消失，则ReplicaSet 会注意到缺少了pod，并创建替代的pod\n\n- label selector（标签选择器），用于确定ReplicaSet域内有哪些pod\n- replica count（副本个数），指定运行的pod的数量\n- pod template（pod 模版），用于创建新的副本\n\n\n\n##### 在kubernetes中运行镜像所发生的步骤\n\n![IMG_8589](/img/IMG_8589.JPG)\n\n\n## 服务发现与负载均衡\n\n### 在kubernetes 中，服务发现与负载均衡问题主要有两种方式：\n\n#### Service\n\n> 前面提到，service有三种类型ClusterIP、NodePort、LoadBalancer\n\n这三种类型的服务都可以用来做服务发现，只不过其职责不同：\n\n- ClusterIP 适用于内部服务的发现，会默认分配一个仅集群内部可访问的IP。这样，内部服务可以通过虚拟IP来访问\n- NodePort 适用于需要将服务暴露到外部的场景， 服务会在每台虚拟机上绑定一个端口，这样就可以通过<NodeIP>:NodePort 的方式来访问该服务\n\n![image-20190716201554956](/img/image-20190716201554956.png)\n\n- LoadBalancer 同上，只用于将服务暴露到外部的场景， 不同的是，这种方式需要依赖cloud provider创建一个外部的负载均衡器，并将请求转发到<NodeIP>:NodePort 上\n\n![image-20190716201543371](/img/image-20190716201543371.png)\n\n#### Ingress Controller\n\nService虽然解决了服务发现和负载均衡的问题，但其有些额外的限制，比如： 对外访问时，NodePort类型需要在外部搭建额外的负载均衡，而LoadBalancer  则要求kubernetes必须跑在cloud provider 上。\n\nIngress 就是为了解决这些限制而引入的新资源（并非service的一种），主要用于将服务暴露到cluster外面，并且可以自定义服务的访问策略。\n\nIngress 本身并不会自动创建负载均衡器，cluster中需要运行一个ingress controller来根据ingress的定义来管理负载均衡器。\n\ningress 原理图：\n\n![image-20190716201523964](/img/image-20190716201523964.png)\n\n\n\n"}]