[{"title":"拆解Http keep-alive","url":"/2019/07/23/拆解Http-keep-alive/","content":"\n# 拆解Http keep-alive\n\n>  最近正好在研究okhttp client，涉及到他的连接管理实现，就顺便以keep-alive的实现机制为线索，拆解了下这部分的实现原理\n\n\n\n# 服务端如何实现keepalive？（以spring boot 内嵌的tomcat为例）\n\n### 服务端请求处理过程\n\n要想知道服务端如何处理keep-alive，就得先了解下他的请求处理过程：\n\n```\nNioEndpoint$Acceptor.run  //轮询处理socket连接，交给processor线程去处理\n  - NioEndpoint$Socketprocessor.doRun  //处理socket read事件，并基于处理完成返回的state处理socket\n    - AbstractProtocol$ConnectionHandler.process //根据协议选择对应的processor，并基于返回的state状态处理processor\n      - AbstractProcessorLight.process\n        - Http11Processor.service //实际的协议处理类，会调用CoyoteAdapter.service拿到reponse，并基于返回值设置response code \n          - CoyoteAdapter.service\n          .\n          .\n          .\n```\n\n### 针对socket状态的处理\n\n从上面的流程中，很容易看出，针对socket的处理实际上是在`NioEndpoint$Socketprocessor.doRun` 中做的，看看具体实现：\n\n```\n                if (handshake == 0) {\n                    SocketState state = SocketState.OPEN;\n                    // Process the request from this socket\n                    if (event == null) {\n                        state = getHandler().process(socketWrapper, SocketEvent.OPEN_READ);\n                    } else {\n                        state = getHandler().process(socketWrapper, event);\n                    }\n                    if (state == SocketState.CLOSED) {\n                        close(socket, key);\n                    }\n                } else if (handshake == -1 ) {\n                    close(socket, key);\n                } else if (handshake == SelectionKey.OP_READ){\n                    socketWrapper.registerReadInterest();\n                } else if (handshake == SelectionKey.OP_WRITE){\n                    socketWrapper.registerWriteInterest();\n                }\n```\n\n从以上代码可以看出，当state 为`SocketState.CLOSED` 时，服务端会主动close Socket，那么close(socket, key)又做了什么事呢？\n\n```\nprivate void close(NioChannel socket, SelectionKey key) {\n        try {\n            if (socket.getPoller().cancelledKey(key) != null) {\n                // SocketWrapper (attachment) was removed from the\n                // key - recycle the key. This can only happen once\n                // per attempted closure so it is used to determine\n                // whether or not to return the key to the cache.\n                // We do NOT want to do this more than once - see BZ\n                // 57340 / 57943.\n                if (running && !paused) {\n                    if (!nioChannels.push(socket)) {\n                        socket.free();\n                    }\n                }\n            }\n        } catch (Exception x) {\n            log.error(\"\",x);\n        }\n    }\n```\n\n其实主要是调用了cancelledKey(key)方法，这个方法主要的作用就是调用channel 的close方法，关闭channel。\n\n\n\n#### 什么情况下socket 会返回SocketState.CLOSED？\n\n从上面的分析，我们知道，当处理线程返回SocketState.CLOSED的时候，客户端与服务端之间建立的channel就会被关闭。那么， 什么情况下，处理线程会反馈SocketState.CLOSED呢？来看看Http11Processor.service方法：\n\n```\n  @Override\n    public SocketState service(SocketWrapperBase<?> socketWrapper)\n        throws IOException {\n        .\n        .\n        .\n            // Process the request in the adapter\n            if (!getErrorState().isError()) {\n                try {\n                    rp.setStage(org.apache.coyote.Constants.STAGE_SERVICE);\n                    getAdapter().service(request, response);\n                    // Handle when the response was committed before a serious\n                    // error occurred.  Throwing a ServletException should both\n                    // set the status to 500 and set the errorException.\n                    // If we fail here, then the response is likely already\n                    // committed, so we can't try and set headers.\n                    if(keepAlive && !getErrorState().isError() && !isAsync() &&\n                            statusDropsConnection(response.getStatus())) {\n                        setErrorState(ErrorState.CLOSE_CLEAN, null);\n                    }\n                } catch (InterruptedIOException e) {\n                    setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e);\n                } catch (HeadersTooLargeException e) {\n                    log.error(sm.getString(\"http11processor.request.process\"), e);\n                    // The response should not have been committed but check it\n                    // anyway to be safe\n                    if (response.isCommitted()) {\n                        setErrorState(ErrorState.CLOSE_NOW, e);\n                    } else {\n                        response.reset();\n                        response.setStatus(500);\n                        setErrorState(ErrorState.CLOSE_CLEAN, e);\n                        response.setHeader(\"Connection\", \"close\"); // TODO: Remove\n                    }\n                } catch (Throwable t) {\n                    ExceptionUtils.handleThrowable(t);\n                    log.error(sm.getString(\"http11processor.request.process\"), t);\n                    // 500 - Internal Server Error\n                    response.setStatus(500);\n                    setErrorState(ErrorState.CLOSE_CLEAN, t);\n                    getAdapter().log(request, response, 0);\n                }\n            }\n\t\t\t\t\t\t.\n\t\t\t\t\t\t.\n\t\t\t\t\t\t.\n        if (getErrorState().isError() || endpoint.isPaused()) {\n            return SocketState.CLOSED;\n        } else if (isAsync()) {\n            return SocketState.LONG;\n        } else if (isUpgrade()) {\n            return SocketState.UPGRADING;\n        } else {\n            if (sendfileState == SendfileState.PENDING) {\n                return SocketState.SENDFILE;\n            } else {\n                if (openSocket) {\n                    if (readComplete) {\n                        return SocketState.OPEN;\n                    } else {\n                        return SocketState.LONG;\n                    }\n                } else {\n                    return SocketState.CLOSED;\n                }\n            }\n        }\n    }\n\n```\n\n在服务端处理没有error的情况下，响应会走到最下面的else里面，可以看到，主要是基于\n\nopenSocket来判断返回的socketstate，而openSocket的状态实际上是由keepAlive的值决定的；SO，实际上，服务端就是基于keepalive的值来判断是否在当次请求结束后关闭连接管道。\n\n\n\n### 如果不关闭，服务端会做什么呢？\n\n从上面的代码可以看到，会有两种情况：当次请求数据已读取完成、当次请求数据读取未完成\n\n- 当次请求数据已经读取完成\n\n  >  返回SocketState.OPEN， 从connections中移除当前socket对应的Processor，并重新给当前socket注册读事件监听\n\n- 当次请求数据读取未完成\n\n  > 返回SocketState.LONG，保留socket对应的Processor，并继续为该socket注册读事件监听\n\n当服务端保留了socket，在未超时的情况，下一次同一个客户端请求过来，就不需要再进行三次握手了。\n\n\n\n## 客户端又是如何处理keep-alive呢？\n\n> 我们以okhttp client 为例，来看看客户端是怎么处理的\n\n### 客户端连接创建过程\n\n如果你了解过okhttp的实现，应该知道okhttp client 的设计是责任链模式。其与服务端的连接建立实际是由ConnectInterceptor 处理的，它会返回一个RealConnection 交给接下来的interceptor去处理。核心处理逻辑是在ExchangeFinder中：\n\n```\n  @Throws(IOException::class)\n  private fun findConnection(\n    connectTimeout: Int,\n    readTimeout: Int,\n    writeTimeout: Int,\n    pingIntervalMillis: Int,\n    connectionRetryEnabled: Boolean\n  ): RealConnection {\n   \t\n    ...\n\n      if (result == null) {\n        // Attempt to get a connection from the pool.\n        if (connectionPool.transmitterAcquirePooledConnection(address, transmitter, null, false)) {\n          foundPooledConnection = true\n          result = transmitter.connection\n        } else if (nextRouteToTry != null) {\n          selectedRoute = nextRouteToTry\n          nextRouteToTry = null\n        } else if (retryCurrentRoute()) {\n          selectedRoute = transmitter.connection!!.route()\n        }\n      }\n    }\n    toClose?.closeQuietly()\n\n    if (releasedConnection != null) {\n      eventListener.connectionReleased(call, releasedConnection!!)\n    }\n    if (foundPooledConnection) {\n      eventListener.connectionAcquired(call, result!!)\n    }\n    if (result != null) {\n      // If we found an already-allocated or pooled connection, we're done.\n      return result!!\n    }\n\n\t\t...\n\n\n    // Do TCP + TLS handshakes. This is a blocking operation.\n    result!!.connect(\n        connectTimeout,\n        readTimeout,\n        writeTimeout,\n        pingIntervalMillis,\n        connectionRetryEnabled,\n        call,\n        eventListener\n    )\n\t\t\n\t\t...\n    \n    return result!!\n  }\n```\n\n切入到transmitterAcquirePooledConnection方法可以看到，如果请求的Address 一样，且有 有效connection 的情况下，默认会返回上一次的连接， 这样就打到了连接复用的目的，也就不需要再进行三次握手。\n\n### 什么情况下，connection会从连接池中移除？\n\n这个就涉及到okhttp的连接管理机制，默认有几种情况：\n\n1、如果连接是非keep-alive模式的，则服务端会在返回头中带上Connection:close（默认500等场景下也会返回close），此时会直接关闭连接，即从connectionPool中删除connection；\n\n2、在keep-alive超时的情况下，也会从connectionPool中删除connection；\n\n3、超过最大空闲连接数量时，从connectionPool中删除connection；\n\n#### by the way\n\n有人可能会担心说，如果默认开启keep-alive连接复用，会不会导致请求会串行发出去呢，其实不会，在okhttp client中，可以设置默认直接支持的连接数，然后他会判断放点已经建立的连接是否是可用状态，比如未读取完成，未传输完成等，都是不能给相同地址的请求复用的，这个时候就会默认新建一个连接。\n\n## 总结\n\n实际上，在http协议中设置keep-alive参数后，客户端会在请求头中带上\"Connection: keep-alive\"，服务端判断到有个参数时，在请求处理完时（一次请求的数据读取完成），在保持连接的open状态，否则关闭连接并在返回的response的header中带上\"Connection: close\"。客户端在收到服务端的响应时，\n会判断response的header中是否有携带\"Connection: close\"，如果有，从连接池中移除当前连接，否则保留当前连接。下次请求同一Address时，会直接基于已有的连接发起，而无需再进行三次握手。","tags":["java"]},{"title":"kubernetes 学习笔记","url":"/2019/07/16/kubernetes-学习笔记/","content":"# 大纲\n\n## kubernetes 介绍\n\n#### kubernetes 集群架构\n\n- master 节点\n\n  > 控制和管理整个集群系统的控制面板\n\n  - Kubernetes API 服务器\n    - Scheduler  调度器，用于调度你的应用\n  - Controller Manager 执行集群级别的功能，如复制组建、持续跟踪工作节点、处理失败节点\n  - etcd 可靠的分布式存储，用于持久化集群配置\n\n- worker 节点\n\n  > 运行容器化应用的机器 \n\n  - kubelet，与API服务器通信，并管理它所在节点的容器\n  - Docker/rtk 容器应用\n  - kube-proxy 负责组件之间的负载均衡网络流量\n\n#### kubernetes 的好处\n\n- 简化应用程序部署\n- \b更好的利用硬件\n- 健康检查和自修复\n- 自动扩容\n\n## 核心组件\n\n### Service\n\n- ClusterIP\n\n  > Kubernetes 的默认服务，定义一个集群内的服务，集群内的其他应用都可以访问该服务。集群外部无法访问它\n\n- LoadBalancer\n\n  > LoadBalancer 服务是暴露服务到internet的标准方式。通过在集群中创建一个负载均衡器来暴露对应服务，它将给你一个单独IP，转发所有流量到你的服务。\n\n- NodePort\n\n  > 将外部流量引导到内部服务的最原始方式，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都会被转发到对应服务\n\n##### 列出所有service\n\n> k get services\n\n\n\n### pod\n\n> 一个pod是一组紧密相关的容器，它们总是一起运行在同一个工作节点上，以及同一个Linux命名空间中，每个pod就像一个独立的逻辑机器，拥有自己的IP、主机名、进程等，运行一个独立的应用程序。\n\n##### 相关命令\n\n> k get pods  //获取pods\n>\n> k logs podname  // 获取指定pod的日志\n\n##### 何时在pod中使用多个容器？\n\n- 他们必须要一起运行\n- 他们代表着一个整体的组件\n- 他们必须一起进行扩容缩容\n\n##### yaml\n\n```yaml\napiVersion: V1\nkind: Pod\nmetadata: \n  name: kubia-manual\nspac:\n  containers:\n  - images: luksa/kubia\n    name: kubia\n    ports:\n    - containerPort: 8080\n    protocol: TCP\n```\n\n##### 标签\n\n> 标签是可以附加到资源的任意键值对，用以选择具有该确切标签的资源，用于快读区分出一组有共同特征的资源\n\n- 命令\n\n  > k get pods  - -show-labels //显示pod 对应的标签\n  >\n  > k get pods -l app=pbkd-user-material-task-pbkd //基于特定标签筛选pod\n\n\n\n##### 命令空间\n\n- 命令\n\n  > k  get ns // 列出所有命名空间\n  >\n  > k get pods -n pbkd // 基于命名空间查找pod\n\n- yaml\n\n  ```\n  apiVersion: V1\n  kind: Namespace\n  metadata: \n    name: kubia-manual\n  ```\n\n\n\n##### 停止和移除pod\n\n- 命令\n\n  > k delete pod kubia  //删除单个pod\n  >\n  > k delete pod - - all  //删除命名空间的所有pod\n  >\n  > k delete add - - all //删除所有资源\n\n##### 存活探针（liveness probe）\n\n> kubernetes 通过存活探针检查容器是否还在运行，可以为pod中的每个容器单独指定存活探针，如果检测失败，kubernetes将定期执行探针并重新启动容器\n\n- 三种探测容器的机制\n  - HTTP GET探针，对容器的IP地址执行HTTP GET请求，如果探测器收到响应，并且响应状态码<400 则代表成功，返回状态码错误或没有返回，则代表失败，重新启动容器\n  - TCP套接字探针尝试与容器指定端口建立TCP连接，如果连接成功建立则探测成功。否则容器重新启动\n  - Exec 探针在容器内执行任意命令，并检查命令的退出码。如果状态码是0 则探测成功。\n\n##### ReplicaSet\n\n> 是一种kubernetes 资源，用户确保pod始终保持运行状态，如果他维护的pod因任何原因消失，则ReplicaSet 会注意到缺少了pod，并创建替代的pod\n\n- label selector（标签选择器），用于确定ReplicaSet域内有哪些pod\n- replica count（副本个数），指定运行的pod的数量\n- pod template（pod 模版），用于创建新的副本\n\n\n\n##### 在kubernetes中运行镜像所发生的步骤\n\n![IMG_8589](/img/IMG_8589.JPG)\n\n\n## 服务发现与负载均衡\n\n### 在kubernetes 中，服务发现与负载均衡问题主要有两种方式：\n\n#### Service\n\n> 前面提到，service有三种类型ClusterIP、NodePort、LoadBalancer\n\n这三种类型的服务都可以用来做服务发现，只不过其职责不同：\n\n- ClusterIP 适用于内部服务的发现，会默认分配一个仅集群内部可访问的IP。这样，内部服务可以通过虚拟IP来访问\n- NodePort 适用于需要将服务暴露到外部的场景， 服务会在每台虚拟机上绑定一个端口，这样就可以通过<NodeIP>:NodePort 的方式来访问该服务\n\n![image-20190716201554956](/img/image-20190716201554956.png)\n\n- LoadBalancer 同上，只用于将服务暴露到外部的场景， 不同的是，这种方式需要依赖cloud provider创建一个外部的负载均衡器，并将请求转发到<NodeIP>:NodePort 上\n\n![image-20190716201543371](/img/image-20190716201543371.png)\n\n#### Ingress Controller\n\nService虽然解决了服务发现和负载均衡的问题，但其有些额外的限制，比如： 对外访问时，NodePort类型需要在外部搭建额外的负载均衡，而LoadBalancer  则要求kubernetes必须跑在cloud provider 上。\n\nIngress 就是为了解决这些限制而引入的新资源（并非service的一种），主要用于将服务暴露到cluster外面，并且可以自定义服务的访问策略。\n\nIngress 本身并不会自动创建负载均衡器，cluster中需要运行一个ingress controller来根据ingress的定义来管理负载均衡器。\n\ningress 原理图：\n\n![image-20190716201523964](/img/image-20190716201523964.png)\n\n\n\n"},{"title":"netty 4 springboot","url":"/2018/08/25/netty-4-springboot/","content":"\n### 概要\n        \n#### 怎么替换掉spring boot 内嵌的tomcat ？ \n    - 首先了解什么是servlet 容器\n        - 什么是Servlet？\n            - 事实上，servlet就是一个Java接口，interface! \n            - 那servlet是干嘛的？很简单，接口的作用是什么？规范呗！\n            > servlet接口定义的是一套处理网络请求的规范，所有实现servlet的类，都需要实现它那五个方法，其中最主要的是两个生命周期方法 init()和destroy()，还有一个处理请求的service()，\n            servlet是一个规范，那实现了servlet的类，就能处理请求了吗？\n            你可以随便谷歌一个servlet的hello world教程，里面都会让你写一个servlet，相信我，你从来不会在servlet中写什么监听8080端口的代码，servlet不会直接和客户端打交道！那请求怎么来到servlet呢？答案是servlet容器，比如我们最常用的tomcat，同样，你可以随便谷歌一个servlet的hello world教程，里面肯定会让你把servlet部署到一个容器中，不然你的servlet压根不会起作用。tomcat才是与客户端直接打交道的家伙，他监听了端口，请求过来后，根据url等信息，确定要将请求交给哪个servlet去处理，然后调用那个servlet的service方法，service方法返回一个response对象，tomcat再把这个response返回给客户端。\n\n        - 所以我们在定义一个servlet 容器时，需要做什么呢？\n            - 首先初始化容器 init方法，由context init\n            - 其次接受请求处理请求。service 方法\n            - 然后能关闭，销毁，destroy\n\n        - servlet 规范的核心接口\n            - ServletContext：定义了一些可以和Servlet Container交互的方法。\n            - Registration：实现Filter和Servlet的动态注册。\n            - ServletRequest(HttpServletRequest)：对HTTP请求消息的封装。\n            - ServletResponse(HttpServletResponse)：对HTTP响应消息的封装。\n            - RequestDispatcher：将当前请求分发给另一个URL，甚至ServletContext以实现进一步的处理。\n            - Servlet(HttpServlet)：所有“服务器小程序”要实现了接口，这些“服务器小程序”重写doGet、doPost、doPut、doHead、doDelete、doOption、doTrace等方法(HttpServlet)以实现响应请求的相关逻辑。\n            - Filter(FilterChain)：在进入Servlet前以及出Servlet以后添加一些用户自定义的逻辑，以实现一些横切面相关的功能\n\n        - 简单看下spring boot 中servlet的处理流程\n            - GenericServlet\n            > servlet 规范的一部分，并不直接关注HTTP，提供了service 方法，接受request，生成reponse。\n            - HttpServlet\n            > 顾名思义，HttpServlet类就是规范中定义的基于HTTP的Servlet实现。用更实际的术语来说，Httpservlet 是一个实现了service\n            方法的抽象类。基于http的请求类型分割方法\n            - HttpServletBean\n            > 接下来，HttpServletBean是层次结构中第一个支持Spring的类。 它使用从web.xml或WebApplicationInitializer收到的servlet \n            init-param值来注入bean的属性。\n            - FrameworkServlet\n            > FrameworkServlet将Servlet功能与Web应用程序上下文集成，实现ApplicationContextAware接口。\n            但它也能够自行创建Web应用程序上下文。\n            - DispatcherServlet\n            > HttpServlet.service 方法的实现类，基于http verb 的类型来路由到不同的controller\n\n    - 再看看embed tomcat 做了什么事\n        - 先介绍几个关键的组件\n            - Endpoint 用来处理底层的socket 网络连接， 即用来实现Tcp/ip 协议\n                - Acceptor 用于监听tcp连接，并将对应的根据状态注册到poller 上\n                - Poller 将socket 添加到poller 队列， 并且轮询pollers events 队列，将关联的套接字交给适当的处理器处理\n                - SocketProcessor 等同于一个Worker，用于从socket中读取数据，最后丢给业务代码处理。\n            - Processor 用于将Endpoint接受到的socket 封装成request， 即用来实现http 协议\n            - Adapter 用于将封装好的request 交给Container 处理，即将请求适配到Servlet 容器进行处理\n\n        - 一个浏览器请求进来的处理过程：\n            0、通过mian 函数，启动内置的servlet 容器，即内置tomcat。\n            1、启动Acceptor 监听tcp连接，有tcp连接时，将socket 注册到Poller 的events 队列里 \n            2、Poller 会轮询 events 队列，当有pollerEvent时，运行pollEvent 的run方法注册一个OP_READ 的事件。（在selector 上注册标记位，标示可读、可写或者有新的连接到来）\n            3、2、Poller.processKey 会处理被标记为OP_READ 或 OP_WRITE的事件，基于给定的状态做不同的处理。实际就是新起一个SocketProcessor处理请求。\n            4、NioEndpoint$SocketProcessor，等同于一个Worker，用于从socket中读取数据，最后丢给业务代码处理。\n            ````\n            Nio2Endpoint$Socketprocessor.doRun  //处理socket read事件，并基于处理完成返回的state处理socket\n                - AbstractProtocol$ConnectionHandler.process //根据协议选择对应的processor，并基于返回的socket状态判断处理processor\n                    - AbstractProcessorLight.process\n                        - Http11ProcessorLight.service    //\b实际的协议处理类，会调用CoyoteAdapter.service拿到reponse，并基于返回值设置response code \n                            - CoyoteAdapter.service\n                                - StrandardEngineValve.invoke\n                                - ErrorReportValve.invoke\n                                - StrandardHostValve.invoke\n                                - AuthenticatorBase.invoke\n                                - StrandardContextValve.invoke\n                                - StrandardWrapperValve.invoke  // 创建一个ApplicationFilterChain 并向其中注入servlet (dispatcherServlet)\n                                    - ApplicationFilterChain.doFilter   //Application FilterChain 首先轮询FilterChain 上面的Filter，执行doFilter 方法，然后调用DispatcherServlet.service 交给spring 框架执行接下来的处理逻辑\n                                    .\n                                    .\n                                    .\n                                    .\n                                        - HttpServlet.service\n                                            - FrameworkServlet.service\n                                            - FrameworkServlet.doService\n                                            - FrameworkServlet.processRequest\n                                                - DispatcherServlet.doService\n                                                    - ServletServerHttpResponse // 封装了HttpServletResponse，通过AbstractHttpMessageConverter 写入到servletResponse 的outputStream中，\n                                                        然后调用flush 方法刷到客户端\n            ````\n            5、当业务代码处理完以后，获取servletResponse的outputstream 将数据写入到buffer 中。\n            6、调用Outputstream 的 flush 方法将数据刷新到socket 中\n\n#### 基于netty 替换embed tomcat 网络层 如何做呢？\n    - 首先看看netty 能干什么\n        - 1、先看看通常我们基于netty写一个http 服务器时，是怎么做的：\n            - Netty 服务端时序图.png\n            - 1、\b创建两个NioEventLoopGroup 实例，一个用于服务端接收客户端的连接，一个用于进行SocketChannel 的网络读写\n            - 2、创建 ServerBootstrap 对象， 它是Netty用于启动NIO服务端的辅助启动类\n            - 3、调用ServerBootstrap 的group方法，将两个NIO线程组当作入参传递到ServerBootStrap 中，接着设置创建的Channel 为 NioServerSocketChannel，\n            它的功能对应于JDK NIO类库中的ServerSocketChannel类。然后配置NioServerSocketChannel 的TCP参数，同时将他的backlog 设置为1034（即Accept queue），\n            最后绑定ChildChannelHandler用于处理I／O事件\n            - 4、调用 ServerBootstrap 的bind 方法绑定监听端口，随后，调用它的同步阻塞方法，sync 等待绑定操作完成。完成之后Netty 会返回一个ChannelFuture，\n            它的功能类似于JDK 的 java.util.concurrent.Future，主要用于异步操作的通知回调\n            - 5、使用f.channel().closeFuture().sync() 方法进行阻塞， 等到服务端链路关闭之后main 函数才退出\n            - 6、调用NIO 线程组的shutdownGracefully 进行优雅退出，他会释放genshutdownGracefully 相关联的资源\n    \n    - 几个关键组件\n        - EmbeddedNettyFactory  // 核心类 ，实现自spring 的 EmbeddedServletContainerFactory，用于生产                 EmbeddedServletContainer， 同时调用ServletContextInitializer 的onstartup 对ServletContext进行初始化配置\n        - NettyContiner  // 用于启动netty 容器 \n            - HttpServerCodec   // HttpRequestDecoder 和 HttpResponseEncoder 的组合，实际就是字节数组 转化为 HttpMessage\n            - ServletContentHandler // 将HttpMessage 封装为 HttpServletRequest 和 HttpServletResponse 方便交给spring 框架做处理\n            - RequestDispatcherHandler  // 将封装好的HttpServletRequest 和 HttpServletResponse 分发给对应的servlet 处理，实际就是dispatcherServlet\n        - NettyServletRegistration // 继承自ServletRegistration.Dynamic 用于向context 中注册 servlet mapping，以及动态注入Servlet\n        - SimpleFilterChain //实现自FilterChain，引用了servlet，在做完filter 逻辑后调用servlet.service \n        - NettyServletRequest   // 实现HttpServletRequest接口，引入ServletInputStream，servletContext 方便用于对输入数据做一些额外处理\n        - NettyServletResponse  // 实现HttpServletRequest接口，引入ServletOutputStream， 主要用于回写业务处理的结果到ServletOutputStream 中\n        - NettyResponseOutputStream  // 继承自ServletOutputStream，引入了ChannelHandlerContext，用语将结果会写到ByteBuf 中\n        - NettyRequestInputStream   // 继承自ServletInputStream，方便对输入流做额外处理\n        "},{"title":"dubbo 深入学习","url":"/2018/06/25/dubbo-深入学习/","content":"\n### 服务端：\n- 暴露服务提供方：ServiceBean\n````\n    AnnotationBean.postProcessAfterInitialization   // 生成serviceConfig，将对应的实现类bean以及interfaceClass塞到serviceConfig中，并调用其export 方法\n    ServiceBean     // 监听ContextRefreshEvent 事件调用ServiceConfig 的export 方法\n    ServiceConfig   // 1、通过proxyFactory 获取到该接口的代理类；2、通过protocol来暴露invoker\n    DubboProtocol   // 1、组装url；2、通过Exchangers注册service\n    Exchangers.export   // 获取Exchanger 调用bind 方法将service 暴露出去\n    HeaderExchangeServer    // 1、构建HeaderExchanger\bHandler 用于接口消息的处理；2、调用Transporters 的bind 方法暴露service\n    Transporters    // 1、获取对应的Transporter，调用其bind 方法暴露service\n    NettyTransporter    // 1、构建NettyServer，初始化的时候会调用doOpen 方法来\n````\n\n- 启动服务消费放：ReferenceBean\n````\n    ReferenceBean   //spring在将其注入某个对象中时，会调用去\b其getObject，getObject方法会调用ReferenceConfig的init方法\n    ReferenceConfig     // 调用DubboProtocol 封装对应的invoker，invoker持有调用的nettyclient\n    DubboProtocol   // 创建invoker 实例，并注入nettyclient\n    ReferenceConfig     // 调用JavassistProxyFactory 的getProxy 方法，创建对应调用service 的代理类，调用代理方法的时候实际调用的就是invocation.invoke\n````\n\n- 服务配置解析：ServerConfig\n- 服务启动：NettyServer 配置了 NettyCodecAdapter 作为编解码的handler，NettyHandler作为实际处理请求的handler。\n- 服务调用：\n    - 服务消费方发起调用：\n    \n````\n        InvokerInvocationHandler    // 持有invoker，调用invoker的invoke 方法发起对远程方法的调用\n        MockClusterInvoker  //  判断是否是mock 调用\n        AbstractClusterInvoker  // 获取LoadBalance，并向下传递\n        FailoverClusterInvoker  //基于LB策略获取对应的invoker\n        InvokerWrapper      // donothing    \n        ProtocolFilterWrapper   // 在实际invoker之前，迭代过滤链\n        ConsumerContextFilter   // 消费RpcContext 上下文\n        FuturnFilter    // 针对同步、异步调用做调用后的callback操作\n        MonitorFilter   // 调用监控数据采集\n        ListenerInvokerWrapper  // 注入listener\n        AbstractInvoker     //\n        DubboInvoker    // 拼装RpcInvocation，并创建DefaultFuture调，如果是同步调用，则调用其get方法阻塞获取调用结果\n        ReferenceCountExchangeClient    //内部维护一个自增引用计数器，每当对象被引用时，计数器就加1\n        HeaderExchangeClient    //封装了心跳检测的逻辑\n        HeaderExchangeChannel   //定义request对象，并将该对象传给NettyClient的send方法，同时实际执行创建DefaultFuture逻辑，将req跟DufaultFuture关联起来\n        AbstractPeer    //\n        AsstractClient //获取NettyChannel，调用NettyChannel的send方法\n        NettyChannel#send    // 调用channel.writeAndFlush将消息写入管道\n        OneToOneEncoder#handleDownstream    //Netty 编码器抽象类\n        ExchangeCodec#encode  //实际的编解码器，用与处理基于dubbo协议的编解码\n````\n\n   - 服务提供方处理调用：\n````\n        ExchangeCodec   // 检测请求头中的魔数与规定的魔数是否相等，提前拦截掉非规则的数据包，并对可读字节数进行检测，方式tcp粘包拆包问题\n        DubboCodec      // 创建request，并通过message解析出对应的 DecodeableRpcInvocation\n        NettyHandler    // 实现自SimpleChannelHandler 用以接收netty处理的io 事件\n        HeartbeatHandler.received   // 判断是否是心跳请求，如果是心跳请求则直接返回结果，如果不是则交给AllChannelHandler 处理\n        AllChannelHandler.received  // 将事件请求派发到线程池中处理\n        HeaderExchangeHandler.received  // 判断消息体，基于消息体对应的实例做分发处理，同时做消息交换，将请求转换成请求相应模式与同步转异步模式\n        DubboProtocol$ExchangeHandlerAdapter.reply   // 基于invoker 调用对应service 的invoke方法\n        ProtocolFilterWrapper$1   // 构建invoker filter chain\n        \bAbstractProxyInvoker   // 调用\bAbstractProxyInvoker 的invoker方法，实际调用抽象的doInvoke 方法\n        JavassistProxyFactory$AbstractProxyInvoker.doInvoke // 基于内部类实现\bAbstractProxyInvoker的invoke 方法\n        Wrapper.invokeMethod    // invokeMethod 实际上就是通过javassist 字节码生成的类代理方法\n        NettyChannel#send   // 向客户端发起请求返回方法调用结果\n        ExchangeCodec#encodeResponse //用于将请求的response编码\n````\n\n   - 服务消费方处理返回结果：\n````    \n        NettyCodecAdapter$InterDecoder#messageReceived  //接受管道的读取消息时间，调用DubboCodec处理消息\n        DubboCodec#decodeBody   // 解码返回数据格式，执行反序列化操作，将返回结果保存到RpcResult中\n        ### 另一个线程(DefaultFuture 中会独立启动一个线程来)\n        DefaultFuture#doReceived //线程轮询FUTURES看是否有就绪的请求，当有服务端请求返回时doReceived方法会被唤起，会做一件事：done.signal唤醒用户线程\n        ### DefaultFuture#get 中阻塞的线程被唤起，继续执行接下来的返回操作\n`````\n\n- 协议解析\n    - NettyCodecAdapter 消息的序列化与反序列化，解决tcp 粘包拆包问题\n    > Dubbo 协议的编解码约定：首先约定16个字节的长度用来存放协议头，调用编号、请求报文长度、序列化方式、版本号等信息全都存在于请求头中；剩余部分用于存放body中，\n     其读取逻辑如下：                                                     \n                        将从管道中传入的字节流放入到一个队列中\n                                        |                    yes\n                判读其可读取长度是否大于等于header+body的长度  ---------> 读取固定长度，进行反序列化\n                                        | \n                                        | no\n                                        | \n                                        v\n                                   等待新的数据过来"},{"title":"线上oom 问题排查","url":"/2017/11/07/线上oom-问题排查/","content":"### 现象\n线上加上异常监控后，11月7号13:21分左右，线上loan-application-service 连续爆出 invoke failed exception，持续将近一分钟，今天抽空查了下这个问题：\n\n### 排查过程\n##### 1、 通过zabbix 查看网络流量 及 cup load 情况。\n![屏幕快照_2016-11-12_下午4.44.31](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/8217a4e17827b4de85871053cc23aa7d/屏幕快照_2016-11-12_下午4.44.31.png)\n![屏幕快照_2016-11-12_下午5.02.30](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/6670ff22fa5b9c510134b33fa0596924/屏幕快照_2016-11-12_下午5.02.30.png)\n![屏幕快照_2016-11-12_下午5.05.41](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/5c57b60abe4d5425c17c507636ee7754/屏幕快照_2016-11-12_下午5.05.41.png)\n\n网络流程正常，但cpu load 明显升高。\n\n##### 2、登陆 kibana 查看 loan-application-service 服务日志\n![屏幕快照_2016-11-12_下午4.10.33](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/3e7b4be12e0bdba2e5321c160d1ead5b/屏幕快照_2016-11-12_下午4.10.33.png)\n定位到有出现OOM。\n\n##### 3、查看gc 日志\n![屏幕快照_2016-11-12_下午4.22.11](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/95baf95ef5ccdaf9f3778facb2f0ed0c/屏幕快照_2016-11-12_下午4.22.11.png)\n定位到那个时间点有频繁的full gc，怀疑内存中有大数组对象导致频繁gc。\n\n##### 4、找DNA排查那个时间段的数据库操作\n![屏幕快照_2016-11-12_下午5.08.33](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/850f44da6bdd220eec107e6e543c3ae1/屏幕快照_2016-11-12_下午5.08.33.png)\n###### 出现如下sql\n````\nSELECT *\nFROM loan_consumer_application\nWHERE 1 = 1\n\tAND applyStatus = 1\n\tAND approveStatus <> 3\n\tAND loanType IN (6, 16)\n````\n这条sql 会查出将近20w条数据。\n\n##### 5、论证一下是不是由这种情况引起的？\n在jvm发生oom 的时候，在jvm 设置了-XX:HeapDumpPath=/data/program/com.wacai.loan/loan-application-provider/1.0.0/java.hprof 的情况下，默认会生成java.hprof 文件，通过MAT，我们可以来分析一下这个文件：\n![屏幕快照_2016-11-12_下午5.54.04](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/0c30cd05d53b4623e9724136930a0e45/屏幕快照_2016-11-12_下午5.54.04.png)\n![屏幕快照_2016-11-12_下午6.06.33](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/61062e51b63b0056096d20949f13cf06/屏幕快照_2016-11-12_下午6.06.33.png)\n![屏幕快照_2016-11-12_下午6.17.38](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/59a56b9d82e5fa4c0685902478b06d0a/屏幕快照_2016-11-12_下午6.17.38.png)\n证实了我们的想法，装在loanapplication 的数组占用了366M的堆内存\n\n##### 6、定位sql 触发点\n![屏幕快照_2016-11-12_下午5.13.20](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/f4b647017461bcde020c2bf52c12ab88/屏幕快照_2016-11-12_下午5.13.20.png)\n   \n![屏幕快照_2016-11-12_下午5.10.49](http://git.caimi-inc.com/finance-loan-rd/loan-application-service/uploads/b7e4f2dece3880e5498542c0b4648107/屏幕快照_2016-11-12_下午5.10.49.png)\n在没有传idno 的情况下，就会出现这种情况。\n\n\n> so，总结下来两个点：\n> - 1、loanApplicationService.queryByParams 方法一定要慎用（当然现在已经让银子加上了limit)。 如果不需必须要用老的申请单服务，建议都迁到新申请单服务上面。\n> - 2、异常监控报警一定要关注，每个报警都是有原因的。"}]